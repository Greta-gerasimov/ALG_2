import math
import numpy as np
import matplotlib.pyplot as plt
# import pandas as pd
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
# from sklearn.metrics import multilabel_confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn import metrics
from sklearn.metrics import f1_score
from matplotlib.colors import ListedColormap
# from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, precision_score, recall_score
np.random.seed(42)

    
def Sigmoid(z):
    return 1/(1 + np.exp(-z))


#making some data
classes = datasets.make_classification(n_samples=10000, n_features=2, n_informative=2,
                                       n_redundant=0, n_classes=2, random_state=42)



shuffle_index = np.random.permutation(classes[0].shape[0])
X_shuffled, y_shuffled = classes[0][shuffle_index], classes[1][shuffle_index]
train_proportion = 0.75
train_test_cut = int(len(classes[0]) * train_proportion)

X_train, X_test, y_train, y_test = \
    X_shuffled[:train_test_cut], \
    X_shuffled[train_test_cut:], \
    y_shuffled[:train_test_cut], \
    y_shuffled[train_test_cut:]
X_train_tr = X_train.transpose()
y_train_tr = y_train.reshape(1, y_train.shape[0])
X_test_tr = X_test.transpose()
y_test_tr = y_test.reshape(1, y_test.shape[0])


ss_train = StandardScaler()
X_train = ss_train.fit_transform(X_train)

ss_test = StandardScaler()
X_test = ss_test.fit_transform(X_test)



def log_loss(w, X, y):
    m = X.shape[1]
    A = Sigmoid(np.dot(w.T, X))
    loss = -1.0 / m * np.sum(y * np.log(A) + (1 - y) * np.log(1 - A))
    loss = np.squeeze(loss)
    grad = 1.0 / m * np.dot(X, (A - y).T)
    
    return loss, grad

def optimize(w, X, y, n_iterations, eta):
    losses = []
    X = np.vstack((np.ones((1,X.shape[1])),X))
    for i in range(n_iterations):        
        loss, grad = log_loss(w, X, y)
        w = w - eta * grad
        losses.append(loss)
        
    return w, losses


def predict(w, X):
    
    m = X.shape[1]
    X = np.vstack((np.ones((1,X.shape[1])),X))
    y_predicted = np.zeros((1, m))
    w = w.reshape(X.shape[0], 1)
    A = Sigmoid(np.dot(w.T, X))
    
    for i in range(A.shape[1]):
        if (A[:,i] > 0.5): 
            y_predicted[:, i] = 1
        elif (A[:,i] <= 0.5):
            y_predicted[:, i] = 0
    return y_predicted


w0 = np.zeros((X_train_tr.shape[0]+1, 1))+0.1

n_iterations = 5000
eta = 0.005

w, losses = optimize(w0, X_train_tr, y_train_tr, n_iterations, eta)

y_predicted_test = predict(w, X_test_tr)
y_predicted_train = predict(w, X_train_tr)


xl = np.array([[1,1],[-2,2],[-2,2]])*0.9
yt = np.dot(w.T, xl)

colors = ListedColormap(['red', 'blue','green'])
y_x = predict(w,classes[0].T)

plt.title('Log loss')
plt.xlabel('iterations')
plt.ylabel('loss')
plt.plot(range(len(losses)), losses)

plt.figure(figsize=(25, 16))
plt.subplot(1,2,1)
plt.scatter([x[0] for x in classes[0]], [x[1] for x in classes[0]], c=classes[1], cmap=colors)
plt.grid()
plt.title('True y')
plt.xlabel('x0')
plt.ylabel('x1')
plt.subplot(1,2,2)
plt.scatter([x[0] for x in classes[0]], [x[1] for x in classes[0]], c=y_x.reshape(-1), cmap=colors)
plt.grid()
plt.title('Predict y')
plt.xlabel('x0')
plt.ylabel('x1')
plt.show()

# print(y_test.shape)
# print(y_predicted_test.shape)
lr_pred = y_predicted_test.transpose()
# print(lr_pred.shape)

#f1_score
from sklearn.metrics import f1_score
f1= f1_score(y_test,  lr_pred ,  average=None, zero_division="warn")
print(f'cреднее гармоническое точности и полноты f1_score:{f1} ')

#доля правильных ответов, точность, полнота алгоритма 
accuracy = accuracy_score(lr_pred, y_test)
precision = precision_score(lr_pred , y_test)
recall = recall_score(lr_pred , y_test)
print(f'доля правильных ответов алгоритма logR: {accuracy}, точность алгоритма logR:  {precision}, полнота алгоритма logR:  {recall} ')

#матрицa ошибок
cm = metrics.confusion_matrix(y_test, lr_pred )
plt.figure(figsize=(9,9))
sns.heatmap(cm,annot = True, fmt='.3f', linewidth=.5,square=True,cmap='Blues_r')
plt.xlabel('Actual label')
plt.ylabel('Predicted label')
all_sample_title = 'Accuracy Score: {0}'.format(accuracy)
plt.title(all_sample_title,size = 15)
plt.show()


#A receiver operating characteristic curve (ROC curve)
fpr, tpr, _ = metrics.roc_curve(y_test,  lr_pred )
auc = metrics.roc_auc_score(y_test, lr_pred)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('A receiver operating characteristic curve')
plt.legend(loc="lower right")
plt.show()
