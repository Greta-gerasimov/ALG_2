{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_DESCENT_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwsNUnymjx2X3mbfvOQPKg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Greta-gerasimov/ALG_2/blob/main/gradient_DESCENT_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#загрузка библиотек\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def mean_squared_error(y, y_predicted):\n",
        "     \n",
        "    # Calculating error\n",
        "    err = np.sum((y -y_predicted)**2) / len(y)\n",
        "    return err\n",
        " \n",
        "# Gradient Descent Function\n",
        "\n",
        "def gradient_descent(x, y,  iterations=100):\n",
        "    n = x.shape[1]\n",
        "    err_arr = []\n",
        "    weights_arr = []\n",
        "    previous_err = None\n",
        "    iterations=iterations\n",
        "    current_weight = np.array([1, 0.5])\n",
        "    alpha = 0.0001\n",
        "    stopping_threshold = 1e-6\n",
        "\n",
        "    # Estimation of optimal parameters\n",
        "    for i in range(iterations):\n",
        "\n",
        "        \n",
        "        # y_pred\n",
        "        y_predicted = np.dot(current_weight, x)\n",
        "         \n",
        "        # current_err\n",
        "        \n",
        "        current_err = mean_squared_error(y, y_predicted)\n",
        "        \n",
        "        if previous_err and abs(previous_err-current_err)<=stopping_threshold:\n",
        "            break\n",
        "        previous_err = current_err\n",
        "\n",
        "        err_arr.append(current_err)\n",
        "        weights_arr.append(current_weight)\n",
        "         \n",
        "        # Calculating the gradients\n",
        "        weight_derivative = -(2/n) * sum(x @ (y -y_predicted))\n",
        "        \n",
        "        #добавляем матричное умножение т.к. weight_derivative - это вектор, который содержит каждую из частных производных\n",
        "      \n",
        "        \n",
        "        # Updating weights\n",
        "        current_weight = current_weight - (alpha * weight_derivative)\n",
        "      \n",
        "        print(f\"Iteration {i+1}: MSE {current_err}, Weight \\\n",
        "        {current_weight}\")\n",
        "                  \n",
        "       \n",
        "        \n",
        "\n",
        "     \n",
        "    return current_weight\n",
        "    \n",
        " \n",
        " \n",
        "def main():\n",
        "     \n",
        "    # Data\n",
        "    X = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "              [1, 1, 2, 5, 3, 0, 5, 10, 1, 2]])\n",
        "    Y = [45, 55, 50, 55, 60, 35, 75, 80, 50, 60]\n",
        "    \n",
        "\n",
        "    estimated_weight = gradient_descent(X, Y,  iterations=100)\n",
        "    print(f\"Estimated Weight: {estimated_weight}\")\n",
        "     \n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZal-hJE2bh_",
        "outputId": "472117a6-a793-4616-8475-3376196b947d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: MSE 3047.75, Weight         [1.0485 0.5485]\n",
            "Iteration 2: MSE 3024.2839540000004, Weight         [1.0967672 0.5967672]\n",
            "Iteration 3: MSE 3001.0426413839004, Weight         [1.14480272 0.64480272]\n",
            "Iteration 4: MSE 2978.023909889072, Weight         [1.19260766 0.69260766]\n",
            "Iteration 5: MSE 2955.225627865021, Weight         [1.24018315 0.74018315]\n",
            "Iteration 6: MSE 2932.645684075983, Weight         [1.28753027 0.78753027]\n",
            "Iteration 7: MSE 2910.2819875054147, Weight         [1.33465012 0.83465012]\n",
            "Iteration 8: MSE 2888.1324671623543, Weight         [1.3815438 0.8815438]\n",
            "Iteration 9: MSE 2866.195071889639, Weight         [1.42821239 0.92821239]\n",
            "Iteration 10: MSE 2844.4677701739556, Weight         [1.47465697 0.97465697]\n",
            "Iteration 11: MSE 2822.94854995771, Weight         [1.52087862 1.02087862]\n",
            "Iteration 12: MSE 2801.635418452707, Weight         [1.5668784 1.0668784]\n",
            "Iteration 13: MSE 2780.5264019556025, Weight         [1.61265739 1.11265739]\n",
            "Iteration 14: MSE 2759.6195456651294, Weight         [1.65821663 1.15821663]\n",
            "Iteration 15: MSE 2738.912913501076, Weight         [1.70355719 1.20355719]\n",
            "Iteration 16: MSE 2718.404587924993, Weight         [1.74868012 1.24868012]\n",
            "Iteration 17: MSE 2698.092669762619, Weight         [1.79358645 1.29358645]\n",
            "Iteration 18: MSE 2677.975278028009, Weight         [1.83827724 1.33827724]\n",
            "Iteration 19: MSE 2658.050549749346, Weight         [1.88275351 1.38275351]\n",
            "Iteration 20: MSE 2638.3166397964187, Weight         [1.92701629 1.42701629]\n",
            "Iteration 21: MSE 2618.7717207097535, Weight         [1.97106661 1.47106661]\n",
            "Iteration 22: MSE 2599.413982531385, Weight         [2.01490549 1.51490549]\n",
            "Iteration 23: MSE 2580.2416326372413, Weight         [2.05853394 1.55853394]\n",
            "Iteration 24: MSE 2561.25289557114, Weight         [2.10195298 1.60195298]\n",
            "Iteration 25: MSE 2542.446012880371, Weight         [2.14516361 1.64516361]\n",
            "Iteration 26: MSE 2523.8192429528563, Weight         [2.18816682 1.68816682]\n",
            "Iteration 27: MSE 2505.370860855867, Weight         [2.23096362 1.73096362]\n",
            "Iteration 28: MSE 2487.0991581762846, Weight         [2.273555 1.773555]\n",
            "Iteration 29: MSE 2469.002442862396, Weight         [2.31594193 1.81594193]\n",
            "Iteration 30: MSE 2451.079039067201, Weight         [2.35812541 1.85812541]\n",
            "Iteration 31: MSE 2433.3272869932157, Weight         [2.40010641 1.90010641]\n",
            "Iteration 32: MSE 2415.745542738773, Weight         [2.4418859 1.9418859]\n",
            "Iteration 33: MSE 2398.332178145786, Weight         [2.48346485 1.98346485]\n",
            "Iteration 34: MSE 2381.085580648971, Weight         [2.52484421 2.02484421]\n",
            "Iteration 35: MSE 2364.0041531265188, Weight         [2.56602496 2.06602496]\n",
            "Iteration 36: MSE 2347.086313752192, Weight         [2.60700804 2.10700804]\n",
            "Iteration 37: MSE 2330.3304958488397, Weight         [2.6477944 2.1477944]\n",
            "Iteration 38: MSE 2313.735147743316, Weight         [2.68838499 2.18838499]\n",
            "Iteration 39: MSE 2297.298732622784, Weight         [2.72878074 2.22878074]\n",
            "Iteration 40: MSE 2281.0197283924044, Weight         [2.7689826 2.2689826]\n",
            "Iteration 41: MSE 2264.896627534379, Weight         [2.80899148 2.30899148]\n",
            "Iteration 42: MSE 2248.9279369683477, Weight         [2.84880832 2.34880832]\n",
            "Iteration 43: MSE 2233.1121779131195, Weight         [2.88843404 2.38843404]\n",
            "Iteration 44: MSE 2217.4478857497324, Weight         [2.92786956 2.42786956]\n",
            "Iteration 45: MSE 2201.9336098858225, Weight         [2.96711578 2.46711578]\n",
            "Iteration 46: MSE 2186.5679136212902, Weight         [3.00617363 2.50617363]\n",
            "Iteration 47: MSE 2171.349374015256, Weight         [3.04504399 2.54504399]\n",
            "Iteration 48: MSE 2156.276581754287, Weight         [3.08372778 2.58372778]\n",
            "Iteration 49: MSE 2141.3481410218888, Weight         [3.12222589 2.62222589]\n",
            "Iteration 50: MSE 2126.5626693692484, Weight         [3.1605392 2.6605392]\n",
            "Iteration 51: MSE 2111.9187975872055, Weight         [3.19866862 2.69866862]\n",
            "Iteration 52: MSE 2097.4151695794653, Weight         [3.23661501 2.73661501]\n",
            "Iteration 53: MSE 2083.050442237009, Weight         [3.27437925 2.77437925]\n",
            "Iteration 54: MSE 2068.823285313723, Weight         [3.31196223 2.81196223]\n",
            "Iteration 55: MSE 2054.7323813032053, Weight         [3.34936482 2.84936482]\n",
            "Iteration 56: MSE 2040.7764253167595, Weight         [3.38658786 2.88658786]\n",
            "Iteration 57: MSE 2026.9541249625577, Weight         [3.42363224 2.92363224]\n",
            "Iteration 58: MSE 2013.2642002259563, Weight         [3.46049881 2.96049881]\n",
            "Iteration 59: MSE 1999.7053833509606, Weight         [3.49718841 2.99718841]\n",
            "Iteration 60: MSE 1986.2764187228236, Weight         [3.53370191 3.03370191]\n",
            "Iteration 61: MSE 1972.9760627517712, Weight         [3.57004014 3.07004014]\n",
            "Iteration 62: MSE 1959.8030837578408, Weight         [3.60620395 3.10620395]\n",
            "Iteration 63: MSE 1946.756261856815, Weight         [3.64219417 3.14219417]\n",
            "Iteration 64: MSE 1933.8343888472627, Weight         [3.67801164 3.17801164]\n",
            "Iteration 65: MSE 1921.0362680986484, Weight         [3.71365718 3.21365718]\n",
            "Iteration 66: MSE 1908.3607144405182, Weight         [3.74913163 3.24913163]\n",
            "Iteration 67: MSE 1895.8065540527502, Weight         [3.78443579 3.28443579]\n",
            "Iteration 68: MSE 1883.372624356849, Weight         [3.8195705 3.3195705]\n",
            "Iteration 69: MSE 1871.0577739082885, Weight         [3.85453656 3.35453656]\n",
            "Iteration 70: MSE 1858.8608622898798, Weight         [3.88933479 3.38933479]\n",
            "Iteration 71: MSE 1846.7807600061642, Weight         [3.92396598 3.42396598]\n",
            "Iteration 72: MSE 1834.8163483788157, Weight         [3.95843094 3.45843094]\n",
            "Iteration 73: MSE 1822.966519443045, Weight         [3.99273048 3.49273048]\n",
            "Iteration 74: MSE 1811.2301758449998, Weight         [4.02686537 3.52686537]\n",
            "Iteration 75: MSE 1799.6062307401396, Weight         [4.06083642 3.56083642]\n",
            "Iteration 76: MSE 1788.0936076925905, Weight         [4.0946444 3.5946444]\n",
            "Iteration 77: MSE 1776.6912405754633, Weight         [4.12829011 3.62829011]\n",
            "Iteration 78: MSE 1765.3980734721215, Weight         [4.16177432 3.66177432]\n",
            "Iteration 79: MSE 1754.2130605784018, Weight         [4.1950978 3.6950978]\n",
            "Iteration 80: MSE 1743.1351661057647, Weight         [4.22826133 3.72826133]\n",
            "Iteration 81: MSE 1732.1633641853768, Weight         [4.26126568 3.76126568]\n",
            "Iteration 82: MSE 1721.296638773108, Weight         [4.2941116 3.7941116]\n",
            "Iteration 83: MSE 1710.5339835554435, Weight         [4.32679986 3.82679986]\n",
            "Iteration 84: MSE 1699.8744018562925, Weight         [4.35933123 3.85933123]\n",
            "Iteration 85: MSE 1689.3169065446905, Weight         [4.39170644 3.89170644]\n",
            "Iteration 86: MSE 1678.8605199433885, Weight         [4.42392624 3.92392624]\n",
            "Iteration 87: MSE 1668.5042737383112, Weight         [4.4559914 3.9559914]\n",
            "Iteration 88: MSE 1658.2472088888906, Weight         [4.48790264 3.98790264]\n",
            "Iteration 89: MSE 1648.0883755392501, Weight         [4.51966071 4.01966071]\n",
            "Iteration 90: MSE 1638.0268329302453, Weight         [4.55126634 4.05126634]\n",
            "Iteration 91: MSE 1628.061649312346, Weight         [4.58272026 4.08272026]\n",
            "Iteration 92: MSE 1618.1919018593478, Weight         [4.6140232 4.1140232]\n",
            "Iteration 93: MSE 1608.4166765829166, Weight         [4.64517589 4.14517589]\n",
            "Iteration 94: MSE 1598.735068247949, Weight         [4.67617904 4.17617904]\n",
            "Iteration 95: MSE 1589.146180288741, Weight         [4.70703338 4.20703338]\n",
            "Iteration 96: MSE 1579.6491247259632, Weight         [4.73773962 4.23773962]\n",
            "Iteration 97: MSE 1570.2430220844278, Weight         [4.76829847 4.26829847]\n",
            "Iteration 98: MSE 1560.9270013116459, Weight         [4.79871064 4.29871064]\n",
            "Iteration 99: MSE 1551.7001996971644, Weight         [4.82897683 4.32897683]\n",
            "Iteration 100: MSE 1542.5617627926727, Weight         [4.85909774 4.35909774]\n",
            "Estimated Weight: [4.85909774 4.35909774]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bQcXCqMv4FoL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}